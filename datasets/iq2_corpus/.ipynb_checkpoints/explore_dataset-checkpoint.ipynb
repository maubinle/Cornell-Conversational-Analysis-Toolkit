{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Dataset\n",
    "## IQ2 Debates\n",
    "*Marianne Aubin Le Quere and Lucas Van Bramer*\n",
    "\n",
    "We are seeking to explore the basic tenets of our dataset. We will later be exploring complexity of language, so we will focus on something that comes quite close to this: word and sentence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get averages\n",
    "First, we want to gather some basic average facts:\n",
    "  * What is the average word length in a debate?\n",
    "  * What is the average sentence length in a debate?\n",
    "  * What is the average utterance length in a debate?\n",
    "  * What is the variability of word, sentence, or utterance length depending on the segment of a debate?\n",
    "  * What is the variability of word, sentence, or utterance length depending on the speaker type of an utterance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules and set up environment\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# replace file path below with your own local convokit\n",
    "os.chdir('/Users/marianneaubin/Documents/Classes/CS6742/Cornell-Conversational-Analysis-Toolkit')\n",
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open created IQ2 corpus\n",
    "corpus = convokit.Corpus(filename='datasets/iq2_corpus/iq2_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 471\n",
      "Number of Utterances: 26562\n",
      "Number of Conversations: 108\n"
     ]
    }
   ],
   "source": [
    "# print basic info about the corpus\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of words per utterance is 71.18\n",
      "average number of words per sentence is 14.61\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# for each utterance, calculate how many words and sentences are in the utterance.\n",
    "utter_ids = corpus.get_utterance_ids()\n",
    "word_counts = []\n",
    "sentence_counts = []\n",
    "num_sentences = 0\n",
    "for utt_id in utter_ids:\n",
    "    utt = corpus.get_utterance(utt_id)\n",
    "    \n",
    "    #we simply use spaces to delineate words\n",
    "    words = utt.text.split()\n",
    "    word_count = len(words)\n",
    "    word_counts.append(word_count)\n",
    "    \n",
    "    #we use regex to separate sentences\n",
    "    sentences = re.split(r'[.!?]+', utt.text)\n",
    "    sentences = list(filter(None, sentences))\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = sentence.split()\n",
    "        words_in_sentence_count = len(words_in_sentence)\n",
    "        sentence_counts.append(words_in_sentence_count)\n",
    "        num_sentences = num_sentences + 1;\n",
    "        \n",
    "# get average word count per utterance\n",
    "word_len_sum = sum(word_counts)\n",
    "utt_num = len(list(corpus.iter_utterances()))\n",
    "avg_word_len = word_len_sum/utt_num\n",
    "print(\"average number of words per utterance is \" + str(round(avg_word_len,2)))\n",
    "\n",
    "# get average word count per sentence\n",
    "word_sent_len_sum = sum(sentence_counts)\n",
    "avg_sentence_len = word_sent_len_sum/num_sentences\n",
    "print(\"average number of words per sentence is \" + str(round(avg_sentence_len, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function get_averages\n",
    "# this function will count the averages of a number of words for a specific scenario\n",
    "#    inputs: list of utterances, instance of corpus\n",
    "#    outputs: average number of words per utterance, average number of words per sentence\n",
    "\n",
    "def get_averages(utt_list, corp): \n",
    "    word_counts = []\n",
    "    sentence_counts = []\n",
    "    num_sentences = 0\n",
    "    for utt_id in utt_list:\n",
    "        utt = corp.get_utterance(utt_id)\n",
    "    \n",
    "        #we simply use spaces to delineate words\n",
    "        words = utt.text.split()\n",
    "        word_count = len(words)\n",
    "        word_counts.append(word_count)\n",
    "        \n",
    "        #we use regex to separate sentences\n",
    "        sentences = re.split(r'[.!?]+', utt.text)\n",
    "        sentences = list(filter(None, sentences))\n",
    "        for sentence in sentences:\n",
    "            words_in_sentence = sentence.split()\n",
    "            words_in_sentence_count = len(words_in_sentence)\n",
    "            sentence_counts.append(words_in_sentence_count)\n",
    "            num_sentences = num_sentences + 1;\n",
    "        \n",
    "    # get average word count per utterance\n",
    "    word_len_sum = sum(word_counts)\n",
    "    utt_num = len(list(corp.iter_utterances()))\n",
    "    avg_word_len = word_len_sum/utt_num\n",
    "    print(\"average number of words per utterance is \" + str(round(avg_word_len,2)))\n",
    "\n",
    "    # get average word count per sentence\n",
    "    word_sent_len_sum = sum(sentence_counts)\n",
    "    avg_sentence_len = word_sent_len_sum/num_sentences\n",
    "    print(\"average number of words per sentence is \" + str(round(avg_sentence_len, 2)))\n",
    "    \n",
    "    return avg_word_len, avg_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function get_segment_utterances\n",
    "# this function takes in a whole corpus and will return a list of utterances\n",
    "# only for the segment specified\n",
    "#    inputs: corpus, desired segment\n",
    "#    outputs: list of utterances in a segment\n",
    "\n",
    "def get_segment_utterances(corp, seg):\n",
    "    seg_utter_ids = []\n",
    "    utter_ids = corp.get_utterance_ids()\n",
    "    for utt_id in utter_ids:\n",
    "        segment = corp.get_utterance(utt_id).meta['segment']\n",
    "        if segment == seg:\n",
    "            seg_utter_ids.append(utt_id)\n",
    "    return seg_utter_ids;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall segment stats:\n",
      "average number of words per utterance is 71.18\n",
      "average number of words per sentence is 14.61\n",
      "\n",
      "intro segment stats:\n",
      "average number of words per utterance is 28.72\n",
      "average number of words per sentence is 15.63\n",
      "\n",
      "discussion segment stats:\n",
      "average number of words per utterance is 32.23\n",
      "average number of words per sentence is 13.73\n",
      "\n",
      "conclusion segment stats:\n",
      "average number of words per utterance is 10.24\n",
      "average number of words per sentence is 14.9\n"
     ]
    }
   ],
   "source": [
    "print(\"overall segment stats:\")\n",
    "avg_word_len, avg_sentence_len = get_averages(corpus.get_utterance_ids(), corpus)\n",
    "\n",
    "print(\"\\nintro segment stats:\")\n",
    "#get intro segment id\n",
    "seg_utt_1 = get_segment_utterances(corpus, 0)\n",
    "avg_word_len_1, avg_sentence_len_1 = get_averages(seg_utt_1, corpus)\n",
    "\n",
    "print(\"\\ndiscussion segment stats:\")\n",
    "#get discussion segment id\n",
    "seg_utt_2 = get_segment_utterances(corpus, 1)\n",
    "avg_word_len_2, avg_sentence_len_2 = get_averages(seg_utt_2, corpus)\n",
    "\n",
    "print(\"\\nconclusion segment stats:\")\n",
    "#get conclusion segment id\n",
    "seg_utt_3 = get_segment_utterances(corpus, 2)\n",
    "avg_word_len_3, avg_sentence_len_3 = get_averages(seg_utt_3, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: get_stance\n",
    "# function that will yield whether a given utterance was from a 'for' 'against' or neutral side\n",
    "#    inputs: utterance\n",
    "#    outputs: 0 for neutral, 1 for 'for', -1 for 'against'\n",
    "\n",
    "def get_stance(utterance):\n",
    "        \n",
    "    stance = utterance.user.meta['stance']\n",
    "    \n",
    "    if stance == 'for':\n",
    "        return 1\n",
    "    elif stance == 'against':\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: get_winner\n",
    "# function that will yield whether a given utterance was from a winner or from a loser\n",
    "#    inputs: utterance\n",
    "#    outputs: 0 for neutral, 1 for winner, -1 for loser\n",
    "\n",
    "def get_winner(corp, utterance):\n",
    "    \n",
    "    print(utterance)\n",
    "    print(utterance.meta)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of for iterances\n",
      "Utterance({'id': '789', 'user': User([('name', 'Christopher Hitchens')]), 'root': '671', 'reply_to': 788, 'timestamp': None, 'text': 'What kind of foolishness is this— “Freedom of expression must include the license to offend”', 'meta': {'nontext': {}, 'segment': 1, 'speakertype': 'for', 'debateid': 'FreedomOfExpression-101806'}})\n",
      "User([('name', 'Christopher Hitchens')])\n",
      "1\n",
      "Utterance({'id': '789', 'user': User([('name', 'Christopher Hitchens')]), 'root': '671', 'reply_to': 788, 'timestamp': None, 'text': 'What kind of foolishness is this— “Freedom of expression must include the license to offend”', 'meta': {'nontext': {}, 'segment': 1, 'speakertype': 'for', 'debateid': 'FreedomOfExpression-101806'}})\n",
      "{'nontext': {}, 'segment': 1, 'speakertype': 'for', 'debateid': 'FreedomOfExpression-101806'}\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_utterance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-21ffc7bdeaaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'789'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_winner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'789'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'789'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#print(len(get_stance(corpus, 'for')))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'get_utterance'"
     ]
    }
   ],
   "source": [
    "# now we try and see if there is a correlation between winning utterances and losing utterances\n",
    "# we also include neutral stances for completion\n",
    "\n",
    "#start with the for side\n",
    "print(\"number of for iterances\")\n",
    "\n",
    "print(corpus.get_utterance('789'))\n",
    "print(get_stance(corpus.get_utterance('789')))\n",
    "print(get_winner(corpus, corpus.get_utterance('789')))\n",
    "print(corpus.conversations.\n",
    "\n",
    "#print(len(get_stance(corpus, 'for')))\n",
    "#avg_num_words_utt_win, avg_num_words_sent_win = get_stance(corpus, 'for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
